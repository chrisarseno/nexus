"""
Production Safety Systems - Real Bias Detection and Ethics Monitoring

This module implements comprehensive production-ready safety systems for AI applications:
- Multi-framework ethics assessment (8 ethical frameworks)
- AI-powered bias detection with severity levels
- Output quarantine system for safety violations
- Real-time safety monitoring and scoring
- Automated safety alerts and recommendations

Adapted from: nexus-system/server/sage/production-safety-systems.ts
"""

import logging
import hashlib
import time
from typing import Dict, List, Any, Optional, Literal
from dataclasses import dataclass, asdict
from datetime import datetime
from enum import Enum

logger = logging.getLogger(__name__)


class BiasType(str, Enum):
    """Types of bias for detection."""
    CONFIRMATION_BIAS = "confirmation_bias"
    SELECTION_BIAS = "selection_bias"
    CULTURAL_BIAS = "cultural_bias"
    GENDER_BIAS = "gender_bias"
    RACIAL_BIAS = "racial_bias"
    ECONOMIC_BIAS = "economic_bias"
    COGNITIVE_BIAS = "cognitive_bias"
    STATISTICAL_BIAS = "statistical_bias"
    NONE_DETECTED = "none_detected"


class BiasType(str, Enum):
    """Ethical framework names."""
    UTILITARIANISM = "utilitarianism"  # Greatest good for greatest number
    DEONTOLOGICAL = "deontological"  # Duty-based ethics
    VIRTUE_ETHICS = "virtue_ethics"  # Character-based ethics
    CARE_ETHICS = "care_ethics"  # Relationships and care
    JUSTICE_THEORY = "justice_theory"  # Fairness and rights
    CONSEQUENTIALISM = "consequentialism"  # Outcome-focused
    PRINCIPLISM = "principlism"  # Four principles approach
    NARRATIVE_ETHICS = "narrative_ethics"  # Context and stories


SeverityLevel = Literal['low', 'medium', 'high', 'critical']


@dataclass
class BiasDetectionResult:
    """Result from bias detection analysis."""
    bias_type: str
    severity: SeverityLevel
    confidence: float
    description: str
    mitigation_suggestions: List[str]
    timestamp: datetime


@dataclass
class EthicsAssessment:
    """Ethics assessment across multiple frameworks."""
    scenario: str
    ethical_frameworks: Dict[str, float]  # Framework -> score (0.0-1.0)
    recommendations: List[str]
    risk_level: float  # 0.0-1.0
    required_actions: List[str]
    timestamp: datetime


@dataclass
class SafetyMonitoringResult:
    """Result from safety monitoring."""
    safety_score: float  # 0.0-1.0
    alerts: List[str]
    recommended_actions: List[str]
    timestamp: datetime


@dataclass
class QuarantineEntry:
    """Entry in the quarantine system."""
    quarantine_id: str
    timestamp: str
    output: str
    reason: str
    hash: str
    severity: SeverityLevel


class ProductionSafetySystem:
    """
    Production Safety System for AI applications.

    Provides comprehensive safety features:
    - Multi-framework ethical assessment
    - AI-powered bias detection
    - Output quarantine for violations
    - Real-time safety monitoring
    """

    def __init__(self, model_adapter=None):
        """
        Initialize the production safety system.

        Args:
            model_adapter: Optional model adapter for AI-powered analysis
        """
        self.model_adapter = model_adapter
        self.bias_history: List[BiasDetectionResult] = []
        self.ethics_history: List[EthicsAssessment] = []
        self.quarantine_entries: List[QuarantineEntry] = []

        self.safety_thresholds = {
            'max_bias_score': 0.3,
            'min_ethics_score': 0.7,
            'quarantine_threshold': 0.8
        }

        logger.info("ðŸ›¡ï¸ Production Safety System initialized")

    def detect_bias(
        self,
        content: str,
        context: Optional[str] = None
    ) -> BiasDetectionResult:
        """
        Detect bias in content using pattern-based analysis.

        For AI-powered detection, use detect_bias_ai() method.

        Args:
            content: Content to analyze for bias
            context: Optional context for analysis

        Returns:
            BiasDetectionResult with detected bias and recommendations
        """
        bias_indicators = {
            BiasType.CONFIRMATION_BIAS: ['only', 'always', 'never', 'everyone knows'],
            BiasType.CULTURAL_BIAS: ['normal', 'weird', 'strange custom', 'exotic'],
            BiasType.GENDER_BIAS: ['hysterical', 'bossy', 'aggressive woman', 'emotional'],
            BiasType.RACIAL_BIAS: ['thugs', 'ghetto', 'articulate', 'exotic'],
            BiasType.ECONOMIC_BIAS: ['poor people', 'lazy', 'entitled', 'welfare'],
            BiasType.COGNITIVE_BIAS: ['obviously', 'clearly', 'everyone agrees', 'common sense'],
        }

        content_lower = content.lower()
        detected_biases = []
        max_severity: SeverityLevel = 'low'
        max_confidence = 0.0

        for bias_type, indicators in bias_indicators.items():
            matches = [ind for ind in indicators if ind in content_lower]
            if matches:
                confidence = min(1.0, len(matches) / len(indicators) * 2)
                if confidence > max_confidence:
                    max_confidence = confidence
                    detected_biases = [bias_type.value]

                    # Determine severity based on number of indicators
                    if len(matches) >= 3:
                        max_severity = 'critical'
                    elif len(matches) >= 2:
                        max_severity = 'high'
                    elif len(matches) >= 1:
                        max_severity = 'medium'

        # Create result
        if detected_biases:
            bias_type = detected_biases[0]
            description = f"Detected {bias_type.replace('_', ' ')} in content"
        else:
            bias_type = BiasType.NONE_DETECTED.value
            description = "No significant bias detected"
            max_severity = 'low'
            max_confidence = 0.1

        result = BiasDetectionResult(
            bias_type=bias_type,
            severity=max_severity,
            confidence=max_confidence,
            description=description,
            mitigation_suggestions=self._generate_bias_mitigations(bias_type, max_severity),
            timestamp=datetime.now()
        )

        self.bias_history.append(result)

        # Log critical bias findings
        if result.severity in ['critical', 'high'] or result.confidence > 0.8:
            logger.warning(f"ðŸš¨ Critical bias detected: {result.bias_type} - {result.description}")

        return result

    async def detect_bias_ai(
        self,
        content: str,
        context: Optional[str] = None
    ) -> BiasDetectionResult:
        """
        Detect bias using AI-powered analysis (requires model adapter).

        Args:
            content: Content to analyze for bias
            context: Optional context for analysis

        Returns:
            BiasDetectionResult with AI-detected bias

        Raises:
            ValueError: If model adapter is not configured
        """
        if not self.model_adapter:
            raise ValueError("Model adapter required for AI-powered bias detection")

        bias_analysis_prompt = f"""Analyze this content for potential bias:

Content: "{content}"
Context: {context or 'General analysis'}

Examine for these bias types:
1. Confirmation bias
2. Selection bias
3. Cultural bias
4. Gender bias
5. Racial bias
6. Economic bias
7. Cognitive bias
8. Statistical bias

Rate severity (low/medium/high/critical) and confidence (0-1).
Return JSON: {{
  "bias_type": "primary bias detected or none_detected",
  "severity": "low|medium|high|critical",
  "confidence": 0.0-1.0,
  "description": "explanation of bias found",
  "mitigation_suggestions": ["suggestion1", "suggestion2"]
}}"""

        try:
            # Use model adapter to analyze
            response = await self.model_adapter.generate(
                prompt=bias_analysis_prompt,
                temperature=0.2,
                max_tokens=500
            )

            # Parse JSON response
            import json
            result_data = json.loads(response.strip().replace('```json', '').replace('```', ''))

            # Validate and create result
            result = BiasDetectionResult(
                bias_type=result_data.get('bias_type', 'none_detected'),
                severity=result_data.get('severity', 'low'),
                confidence=max(0.0, min(1.0, result_data.get('confidence', 0.5))),
                description=result_data.get('description', 'No significant bias detected'),
                mitigation_suggestions=result_data.get('mitigation_suggestions', []),
                timestamp=datetime.now()
            )

            self.bias_history.append(result)

            # Log critical findings
            if result.severity in ['critical', 'high'] or result.confidence > 0.8:
                logger.warning(f"ðŸš¨ AI-detected critical bias: {result.bias_type} - {result.description}")

            return result

        except Exception as error:
            logger.error(f"Bias detection failed: {error}")
            return BiasDetectionResult(
                bias_type='detection_error',
                severity='low',
                confidence=0.1,
                description='Bias detection system encountered an error',
                mitigation_suggestions=['Retry bias analysis', 'Use manual review'],
                timestamp=datetime.now()
            )

    def _generate_bias_mitigations(self, bias_type: str, severity: SeverityLevel) -> List[str]:
        """Generate mitigation suggestions for detected bias."""
        base_suggestions = {
            'confirmation_bias': [
                'Actively seek contradicting evidence',
                'Consider alternative viewpoints',
                'Avoid absolute language'
            ],
            'cultural_bias': [
                'Include diverse cultural perspectives',
                'Avoid culturally-specific assumptions',
                'Use neutral, inclusive language'
            ],
            'gender_bias': [
                'Use gender-neutral language',
                'Avoid gender stereotypes',
                'Consider unconscious bias training'
            ],
            'racial_bias': [
                'Review for racially-coded language',
                'Ensure diverse representation',
                'Conduct bias audit'
            ],
            'economic_bias': [
                'Avoid economic stereotypes',
                'Consider diverse economic backgrounds',
                'Use empathetic language'
            ],
            'cognitive_bias': [
                'Provide evidence for claims',
                'Avoid assuming universal agreement',
                'Acknowledge different perspectives'
            ]
        }

        suggestions = base_suggestions.get(bias_type, ['Review content for potential bias'])

        if severity in ['high', 'critical']:
            suggestions.insert(0, 'URGENT: Review and revise content immediately')

        return suggestions

    async def assess_ethics(
        self,
        scenario: str,
        stakeholders: Optional[List[str]] = None
    ) -> EthicsAssessment:
        """
        Assess scenario using multiple ethical frameworks.

        Args:
            scenario: Scenario to assess
            stakeholders: Optional list of stakeholders affected

        Returns:
            EthicsAssessment with multi-framework evaluation
        """
        stakeholders = stakeholders or []
        ethical_frameworks = [
            'utilitarianism',
            'deontological',
            'virtue_ethics',
            'care_ethics',
            'justice_theory',
            'consequentialism',
            'principlism',
            'narrative_ethics'
        ]

        framework_scores: Dict[str, float] = {}
        recommendations: List[str] = []

        # Simple heuristic-based assessment for each framework
        # In production, this would use AI model for more sophisticated analysis
        for framework in ethical_frameworks:
            score = self._evaluate_framework_heuristic(scenario, framework, stakeholders)
            framework_scores[framework] = score

            # Generate recommendations based on low scores
            if score < 0.5:
                recommendations.append(f"Consider {framework.replace('_', ' ')} implications")

        # Calculate overall ethics score
        overall_score = sum(framework_scores.values()) / len(framework_scores)

        # Determine risk level and required actions
        risk_level = 1.0 - overall_score
        required_actions: List[str] = []

        if overall_score < self.safety_thresholds['min_ethics_score']:
            required_actions.append('Requires ethical review before implementation')
            if overall_score < 0.5:
                required_actions.append('Consider alternative approaches')
            if overall_score < 0.3:
                required_actions.append('DO NOT IMPLEMENT - Major ethical concerns')

        assessment = EthicsAssessment(
            scenario=scenario,
            ethical_frameworks=framework_scores,
            recommendations=list(set(recommendations)),  # Remove duplicates
            risk_level=risk_level,
            required_actions=required_actions,
            timestamp=datetime.now()
        )

        self.ethics_history.append(assessment)

        logger.info(f"âš–ï¸ Ethics assessment complete: {overall_score * 100:.1f}% ethical acceptability")

        return assessment

    def _evaluate_framework_heuristic(
        self,
        scenario: str,
        framework: str,
        stakeholders: List[str]
    ) -> float:
        """
        Evaluate scenario using heuristic for specific ethical framework.

        This is a simplified heuristic. In production, use AI model for sophisticated analysis.
        """
        scenario_lower = scenario.lower()
        score = 0.7  # Default neutral score

        # Framework-specific keyword scoring
        if framework == 'utilitarianism':
            if 'benefit' in scenario_lower or 'majority' in scenario_lower:
                score += 0.2
            if 'harm' in scenario_lower or 'minority' in scenario_lower:
                score -= 0.2

        elif framework == 'deontological':
            if 'duty' in scenario_lower or 'obligation' in scenario_lower or 'rule' in scenario_lower:
                score += 0.2
            if 'exception' in scenario_lower or 'violate' in scenario_lower:
                score -= 0.2

        elif framework == 'virtue_ethics':
            if 'honest' in scenario_lower or 'fair' in scenario_lower or 'compassionate' in scenario_lower:
                score += 0.2
            if 'dishonest' in scenario_lower or 'unfair' in scenario_lower:
                score -= 0.2

        elif framework == 'care_ethics':
            if 'relationship' in scenario_lower or 'care' in scenario_lower or 'support' in scenario_lower:
                score += 0.2
            if 'abandon' in scenario_lower or 'neglect' in scenario_lower:
                score -= 0.2

        elif framework == 'justice_theory':
            if 'fair' in scenario_lower or 'equal' in scenario_lower or 'rights' in scenario_lower:
                score += 0.2
            if 'discrimination' in scenario_lower or 'unjust' in scenario_lower:
                score -= 0.2

        # Consider stakeholder count
        if len(stakeholders) > 0:
            score += 0.1  # Stakeholder consideration is positive

        return max(0.0, min(1.0, score))

    def monitor_safety(self, system_state: Dict[str, Any]) -> SafetyMonitoringResult:
        """
        Monitor overall system safety in real-time.

        Args:
            system_state: Current system state with metrics

        Returns:
            SafetyMonitoringResult with safety score and alerts
        """
        alerts: List[str] = []
        recommended_actions: List[str] = []
        safety_score = 1.0

        # Check recent bias detections
        recent_bias = self.bias_history[-10:] if len(self.bias_history) >= 10 else self.bias_history
        critical_bias = [
            b for b in recent_bias
            if b.severity in ['critical', 'high']
        ]

        if critical_bias:
            safety_score -= 0.2 * len(critical_bias)
            alerts.append(f"{len(critical_bias)} critical bias detections in recent activity")
            recommended_actions.append('Review and address bias in AI outputs')

        # Check ethics compliance
        recent_ethics = self.ethics_history[-5:] if len(self.ethics_history) >= 5 else self.ethics_history
        low_ethics_scores = [e for e in recent_ethics if e.risk_level > 0.5]

        if low_ethics_scores:
            safety_score -= 0.3 * len(low_ethics_scores)
            alerts.append('Multiple scenarios with elevated ethical risk detected')
            recommended_actions.append('Conduct ethics review of system decisions')

        # Monitor system resource usage
        cpu_usage = system_state.get('cpu_usage', 0.0)
        memory_usage = system_state.get('memory_usage', 0.0)

        if cpu_usage > 0.9:
            safety_score -= 0.1
            alerts.append('High CPU usage detected')
            recommended_actions.append('Consider reducing AI model complexity')

        if memory_usage > 0.9:
            safety_score -= 0.15
            alerts.append('High memory usage detected')
            recommended_actions.append('Implement memory optimization strategies')

        result = SafetyMonitoringResult(
            safety_score=max(0.0, safety_score),
            alerts=alerts,
            recommended_actions=recommended_actions,
            timestamp=datetime.now()
        )

        return result

    def quarantine_output(self, output: str, reason: str, severity: SeverityLevel = 'high') -> bool:
        """
        Quarantine problematic AI output.

        In production, this would:
        1. Store quarantined output in secure database
        2. Prevent output from being shown to users
        3. Log the incident for review
        4. Potentially trigger model retraining

        Args:
            output: Output to quarantine
            reason: Reason for quarantine
            severity: Severity level of the issue

        Returns:
            True if quarantine successful
        """
        logger.warning(f"ðŸš« QUARANTINE: Output quarantined due to: {reason}")

        # Create quarantine entry
        quarantine_entry = QuarantineEntry(
            quarantine_id=f"quarantine_{int(time.time() * 1000)}",
            timestamp=datetime.now().isoformat(),
            output=output[:200] + '...' if len(output) > 200 else output,  # Truncate for logging
            reason=reason,
            hash=self._hash_string(output),
            severity=severity
        )

        self.quarantine_entries.append(quarantine_entry)

        # Keep quarantine list manageable
        if len(self.quarantine_entries) > 1000:
            self.quarantine_entries = self.quarantine_entries[-1000:]

        logger.info(f"ðŸ“‹ Quarantine entry created: {quarantine_entry.quarantine_id}")

        return True

    def _hash_string(self, text: str) -> str:
        """Generate hash for string."""
        return hashlib.sha256(text.encode()).hexdigest()[:16]

    def get_safety_stats(self) -> Dict[str, Any]:
        """
        Get safety system statistics.

        Returns:
            Dictionary with safety statistics
        """
        severity_weights = {'low': 1, 'medium': 2, 'high': 3, 'critical': 4}

        avg_bias_severity_score = (
            sum(severity_weights[b.severity] for b in self.bias_history) / len(self.bias_history)
            if self.bias_history else 0.0
        )

        avg_bias_severity = (
            'low' if avg_bias_severity_score < 1.5
            else 'medium' if avg_bias_severity_score < 2.5
            else 'high'
        )

        avg_ethics_score = (
            sum(1.0 - e.risk_level for e in self.ethics_history) / len(self.ethics_history)
            if self.ethics_history else 0.8
        )

        return {
            'bias_detections': len(self.bias_history),
            'ethics_assessments': len(self.ethics_history),
            'average_bias_severity': avg_bias_severity,
            'average_ethics_score': avg_ethics_score,
            'quarantine_count': len(self.quarantine_entries)
        }

    def get_recent_alerts(self, limit: int = 10) -> List[Dict[str, Any]]:
        """
        Get recent safety alerts.

        Args:
            limit: Maximum number of alerts to return

        Returns:
            List of recent safety alerts
        """
        alerts: List[Dict[str, Any]] = []

        # Add critical bias detections
        critical_biases = [
            b for b in self.bias_history[-limit:]
            if b.severity in ['critical', 'high']
        ]
        for bias in critical_biases:
            alerts.append({
                'type': 'bias',
                'severity': bias.severity,
                'description': bias.description,
                'timestamp': bias.timestamp.isoformat()
            })

        # Add high-risk ethics assessments
        high_risk_ethics = [
            e for e in self.ethics_history[-limit:]
            if e.risk_level > 0.5
        ]
        for ethics in high_risk_ethics:
            alerts.append({
                'type': 'ethics',
                'severity': 'high' if ethics.risk_level > 0.7 else 'medium',
                'description': f"High ethical risk: {ethics.scenario[:100]}",
                'timestamp': ethics.timestamp.isoformat()
            })

        # Sort by timestamp and limit
        alerts.sort(key=lambda x: x['timestamp'], reverse=True)
        return alerts[:limit]

    def cleanup(self) -> None:
        """Clean up old data to manage memory."""
        # Keep only recent history
        max_history = 100

        if len(self.bias_history) > max_history:
            self.bias_history = self.bias_history[-max_history:]

        if len(self.ethics_history) > max_history:
            self.ethics_history = self.ethics_history[-max_history:]

        if len(self.quarantine_entries) > max_history:
            self.quarantine_entries = self.quarantine_entries[-max_history:]

        logger.info("ðŸ§¹ Production safety system cleaned up")
